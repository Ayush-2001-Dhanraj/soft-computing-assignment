{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bad9cde",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch [Fashion MNIST](https://nnfs.io/datasets/fashion_mnist_images.zip) - Assignment\n",
    "\n",
    "### Submitted By: \n",
    "> **Ayush Dhanraj**  - 2023DS06  \n",
    "**Ashutosh Kumar** - 2023DS26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c34a81",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170d3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37878fe",
   "metadata": {},
   "source": [
    "#### Download &rarr; Split &rarr; Scale &rarr; Reshape &rarr; Shuffle &rarr; Verify \n",
    "🧹 Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d503e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "784\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_mnist_dataset(dataset, path):\n",
    "    \"\"\"\n",
    "    returns samples and lables specified by path and dataset params\n",
    "\n",
    "    loop through each label and append image to X and label to y\n",
    "    \"\"\"\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label in labels:\n",
    "        image_counter = 0\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    \"\"\"\n",
    "    returns train X, y and test X and y\n",
    "    \"\"\"\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    \n",
    "    return X, y, X_test, y_test\n",
    "\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "\"\"\"\n",
    "Current range 0 - 255\n",
    "Scale in range -1 to 1 centered to 0\n",
    "\"\"\"\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\"\"\"\n",
    "pixel size - 28 x 28 - 784 features\n",
    "images are reshaped to be next to each other\n",
    "or\n",
    "Reshape to vectors (1 - D Array aka list)\n",
    "\"\"\"\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "print(X.shape[1])\n",
    "\n",
    "\"\"\"\n",
    "shuffle the keys to remove biase that might be caused towards one label\n",
    "\"\"\"\n",
    "keys = np.array(range(X.shape[0]))\n",
    "print(keys[:10])\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "plt.imshow((X[4].reshape(28, 28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd75f07",
   "metadata": {},
   "source": [
    "#### 📚 Dense Layer aka Fully Connect Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cdc086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, \n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0, \n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \"\"\"\n",
    "        n_inputs represents the no of features\n",
    "        n_neurons represents the no of neurons we want in this particular layer\n",
    "\n",
    "        weights are inputs x no of neurons - helps avoid transpose operation in dot product\n",
    "        weights range between -1 to +1 so that they are close to zero and NN doesn't explode\n",
    "        \n",
    "        biases is set to zero initially and if we encounter error where the entire output of\n",
    "        NN is zero we can intialize it to some value to avoid dead ANN\n",
    "        \n",
    "        rest four parameters referes to lambda that will be used for\n",
    "        L1 and L2 regularization\n",
    "        \"\"\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        used in backpropagation\n",
    "        dvalues is gradient from next layer\n",
    "        \"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradients - regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e662bd4",
   "metadata": {},
   "source": [
    "#### 🏃‍♂️ Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42ba9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \"\"\"\n",
    "    Almost Linear - fast & accurate\n",
    "    mostly used in hidden layers\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # input negative - gradient zero\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3214f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \"\"\"\n",
    "    exponent & normalize\n",
    "    common classification activation function\n",
    "    mostly used in output layer\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb360288",
   "metadata": {},
   "source": [
    "#### 📉 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70c1ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    common loss class\n",
    "    calculates mean after sample loss is calculated\n",
    "    calculates regularization if specifies during layer Instantiation\n",
    "    \"\"\"\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        mean_loss = np.mean(sample_losses)\n",
    "        return mean_loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \"\"\"\n",
    "    common loss function for multi - class classification problems\n",
    "    \"\"\"\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        takes in predicted and expected output\n",
    "        accounts for zero values -log(0) = inf and then remove biasis caused by its intro\n",
    "        returns - log (correct_confidences)\n",
    "        \"\"\"\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            # y_true is in the form of [1, 0, 1, 1 ....]\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # y_true is in the form of matric [[0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0]....]\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        negative_log_likelihood = - np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \"\"\"\n",
    "        takes gradient from next layer\n",
    "        returns gradient wrt to inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        # convert to hot vector if not\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d01da",
   "metadata": {},
   "source": [
    "#### 🤝 Aggregate - Softmax & Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdced4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \"\"\"\n",
    "    forward - calculates activation output\n",
    "    backpropagation - calculate gradient of softmax output wrt loss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()   \n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b5ea0",
   "metadata": {},
   "source": [
    "#### Optimizers: 🙂 SGD 😃 SGD Momentum 😁 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c449418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1.0, decay=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        weight_updates = -self.current_learning_rate * layer.dweights\n",
    "        bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_SGD_Momentum:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1.0, decay=0, momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        # create momentum array if not present along with biases\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "        weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "        layer.weight_momentums = weight_updates\n",
    "\n",
    "        bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "        layer.bias_momentums = bias_updates\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "    \n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86015903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Model(epochs, X, y, dense1, dense2, dense3, activation1, activation2, loss_activation, optimizer):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    learning_rate = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dense3.forward(activation2.output)\n",
    "        data_loss = loss_activation.forward(dense3.output, y)\n",
    "\n",
    "        regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2) + \\\n",
    "            loss_activation.loss.regularization_loss(dense3)\n",
    "\n",
    "        loss = data_loss + regularization_loss\n",
    "\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions==y)\n",
    "\n",
    "\n",
    "        if not epoch % 10:\n",
    "            accuracies.append(accuracy)\n",
    "            losses.append(loss)\n",
    "            learning_rate.append(optimizer.current_learning_rate)\n",
    "            print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}'\n",
    "             + f' Rr: {regularization_loss}')\n",
    "\n",
    "        loss_activation.backward(loss_activation.output, y)\n",
    "        dense3.backward(loss_activation.dinputs)\n",
    "        activation2.backward(dense3.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        optimizer.post_update_params()\n",
    "        \n",
    "    return accuracies, losses, learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0db318f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(X.shape[1], 128, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(128, 128)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(128, 10)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD(decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3ada434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.063, loss: 2.308, lr: 1.0 Rr: 0.005009538056412376\n",
      "epoch: 10, acc: 0.312, loss: 1.786, lr: 0.9174311926605504 Rr: 0.005340063064569224\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_Model(\u001b[38;5;241m1100\u001b[39m, X, y, dense1, dense2, dense3, activation1, activation2, loss_activation, optimizer)\n",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m, in \u001b[0;36mtrain_Model\u001b[1;34m(epochs, X, y, dense1, dense2, dense3, activation1, activation2, loss_activation, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m dense1\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m      8\u001b[0m activation1\u001b[38;5;241m.\u001b[39mforward(dense1\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m----> 9\u001b[0m dense2\u001b[38;5;241m.\u001b[39mforward(activation1\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m     10\u001b[0m activation2\u001b[38;5;241m.\u001b[39mforward(dense2\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m     11\u001b[0m dense3\u001b[38;5;241m.\u001b[39mforward(activation2\u001b[38;5;241m.\u001b[39moutput)\n",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m, in \u001b[0;36mLayer_Dense.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_Model(1100, X, y, dense1, dense2, dense3, activation1, activation2, loss_activation, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(0, 1100, 10) \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, accuracies_sgd, label='Accuracy', marker='o', color=\"blue\")\n",
    "plt.plot(epochs, losses_sgd, label='Loss', marker='o', color='red')\n",
    "plt.plot(epochs, learning_rate, label='Learning Rate', marker='o', color='black')\n",
    "plt.title('Training Metrics over Epochs - SGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('training_metrics_plot.png')\n",
    "plt.show()\n",
    "\n",
    "epochs = range(0, 800, 10) \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, accuracies_sgd[30:], label='Accuracy', marker='o', color=\"blue\")\n",
    "plt.plot(epochs, losses_sgd[30:], label='Loss', marker='o', color='red')\n",
    "plt.plot(epochs, learning_rate[30:], label='Learning Rate', marker='o', color='black')\n",
    "plt.title('Training Metrics over Epochs - SGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('training_metrics_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a848c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "data_loss = loss_activation.forward(dense3.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation - SGD, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
